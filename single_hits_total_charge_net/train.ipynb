{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from i3cols_dataloader import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gc\n",
    "\n",
    "from transformations import trafo_indep, trafo_q\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12., 8.]\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_hits, repeated_params, total_charge, params, labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_hits_train, single_hits_test, repeated_params_train, repeated_params_test = train_test_split(single_hits,repeated_params, test_size=0.01, random_state=42)\n",
    "total_charge_train, total_charge_test, params_train, params_test = train_test_split(total_charge, params, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LossHistory(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self):\n",
    "#         self.train_losses = []\n",
    "#         self.test_losses = []\n",
    "\n",
    "#     def on_train_batch_end(self, batch, logs={}):\n",
    "#         self.train_losses.append(logs.get('loss'))\n",
    "        \n",
    "#     def on_test_batch_end(self, batch, logs={}):\n",
    "#         self.test_losses.append(logs.get('loss'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trafo_indep_7 (trafo_indep)     (None, 15)           0           input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 32)           512         trafo_indep_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 64)           2112        dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 64)           0           dense_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 128)          8320        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 128)          0           dense_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 256)          33024       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 256)          0           dense_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 128)          32896       dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 128)          0           dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 64)           8256        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 32)           2080        dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 1)            33          dense_86[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 87,233\n",
      "Trainable params: 87,233\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activation='relu'\n",
    "\n",
    "hits_input = tf.keras.Input(shape=(5,))\n",
    "params_input = tf.keras.Input(shape=(len(labels),))\n",
    "t = trafo_indep(labels=labels)\n",
    "\n",
    "h = t(hits_input, params_input)\n",
    "h = tf.keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(128, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(256, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(128, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "hit_net = tf.keras.Model(inputs=[hits_input, params_input], outputs=outputs)\n",
    "hit_net.summary()\n",
    "\n",
    "#my_history = LossHistory()\n",
    "\n",
    "loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trafo_q_1 (trafo_q)             (None, 9)            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           320         trafo_q_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           2112        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          8320        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          33024       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           8256        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           2080        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 87,041\n",
      "Trainable params: 87,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activation='relu'\n",
    "\n",
    "\n",
    "charge_input = tf.keras.Input(shape=(1,))\n",
    "params_input = tf.keras.Input(shape=(len(labels),))\n",
    "t = trafo_q(labels=labels)\n",
    "\n",
    "h = t(charge_input, params_input)\n",
    "h = tf.keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(128, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(256, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "#h = tf.keras.layers.Dense(256, activation=\"relu\")(h)\n",
    "#h = tf.keras.layers.Dropout(0.001)(h)\n",
    "#h = tf.keras.layers.Dense(256, activation=\"relu\")(h)\n",
    "#h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(128, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dropout(0.001)(h)\n",
    "h = tf.keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "h = tf.keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "charge_net = tf.keras.Model(inputs=[charge_input, params_input], outputs=outputs)\n",
    "charge_net.summary()\n",
    "\n",
    "#my_history_q = LossHistory()\n",
    "\n",
    "loss_history_q = []\n",
    "val_loss_history_q = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "hit_net.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "charge_net.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(hits, params, each_row=True):\n",
    "    shuffled_hits = np.vstack([hits, hits])\n",
    "    if each_row:\n",
    "        shuf = np.empty_like(params)\n",
    "        for col in range(params.shape[1]):\n",
    "            indices = np.random.permutation(np.arange(params.shape[0]))\n",
    "            shuf[:, col] = params[indices, col]\n",
    "    else:\n",
    "        shuf = np.random.permutation(params)\n",
    "    shuffled_params = np.vstack([params, shuf])\n",
    "    target = np.concatenate([np.ones(hits.shape[0]), np.zeros(hits.shape[0])])\n",
    "    return shuffled_hits, shuffled_params, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.3651 - val_loss: 0.3432\n",
      "Epoch 2\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.3390 - val_loss: 0.3189\n",
      "Epoch 3\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.3202 - val_loss: 0.3049\n",
      "Epoch 4\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.3086 - val_loss: 0.2962\n",
      "Epoch 5\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.3001 - val_loss: 0.2902\n",
      "Epoch 6\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2935 - val_loss: 0.2835\n",
      "Epoch 7\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2871 - val_loss: 0.2754\n",
      "Epoch 8\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2811 - val_loss: 0.2703\n",
      "Epoch 9\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2762 - val_loss: 0.2670\n",
      "Epoch 10\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2722 - val_loss: 0.2637\n",
      "Epoch 11\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2689 - val_loss: 0.2611\n",
      "Epoch 12\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2658 - val_loss: 0.2593\n",
      "Epoch 13\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2634 - val_loss: 0.2556\n",
      "Epoch 14\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2611 - val_loss: 0.2536\n",
      "Epoch 15\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2592 - val_loss: 0.2516\n",
      "Epoch 16\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2574 - val_loss: 0.2497\n",
      "Epoch 17\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2558 - val_loss: 0.2490\n",
      "Epoch 18\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 68s 3us/sample - loss: 0.2542 - val_loss: 0.2474\n",
      "Epoch 19\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2529 - val_loss: 0.2477\n",
      "Epoch 20\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2516 - val_loss: 0.2462\n",
      "Epoch 21\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2504 - val_loss: 0.2433\n",
      "Epoch 22\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2492 - val_loss: 0.2454\n",
      "Epoch 23\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2482 - val_loss: 0.2419\n",
      "Epoch 24\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2472 - val_loss: 0.2415\n",
      "Epoch 25\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2463 - val_loss: 0.2419\n",
      "Epoch 26\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2456 - val_loss: 0.2400\n",
      "Epoch 27\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2447 - val_loss: 0.2396\n",
      "Epoch 28\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2440 - val_loss: 0.2382\n",
      "Epoch 29\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2435 - val_loss: 0.2391\n",
      "Epoch 30\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2427 - val_loss: 0.2380\n",
      "Epoch 31\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2422 - val_loss: 0.2377\n",
      "Epoch 32\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2414 - val_loss: 0.2392\n",
      "Epoch 33\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2410 - val_loss: 0.2368\n",
      "Epoch 34\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2406 - val_loss: 0.2345\n",
      "Epoch 35\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2404 - val_loss: 0.2353\n",
      "Epoch 36\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2396 - val_loss: 0.2363\n",
      "Epoch 37\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2393 - val_loss: 0.2359\n",
      "Epoch 38\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 68s 3us/sample - loss: 0.2389 - val_loss: 0.2353\n",
      "Epoch 39\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2385 - val_loss: 0.2345\n",
      "Epoch 40\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2380 - val_loss: 0.2329\n",
      "Epoch 41\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2378 - val_loss: 0.2323\n",
      "Epoch 42\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2377 - val_loss: 0.2322\n",
      "Epoch 43\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2373 - val_loss: 0.2319\n",
      "Epoch 44\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 67s 3us/sample - loss: 0.2369 - val_loss: 0.2325\n",
      "Epoch 45\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2367 - val_loss: 0.2312\n",
      "Epoch 46\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2362 - val_loss: 0.2304\n",
      "Epoch 47\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2360 - val_loss: 0.2324\n",
      "Epoch 48\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2358 - val_loss: 0.2321\n",
      "Epoch 49\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2355 - val_loss: 0.2310\n",
      "Epoch 50\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2351 - val_loss: 0.2317\n",
      "Epoch 51\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2349 - val_loss: 0.2294\n",
      "Epoch 52\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2347 - val_loss: 0.2304\n",
      "Epoch 53\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2345 - val_loss: 0.2305\n",
      "Epoch 54\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2343 - val_loss: 0.2304\n",
      "Epoch 55\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2340 - val_loss: 0.2307\n",
      "Epoch 56\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2338 - val_loss: 0.2287\n",
      "Epoch 57\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2336 - val_loss: 0.2287\n",
      "Epoch 58\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 66s 3us/sample - loss: 0.2335 - val_loss: 0.2292\n",
      "Epoch 59\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2333 - val_loss: 0.2294\n",
      "Epoch 60\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2330 - val_loss: 0.2291\n",
      "Epoch 61\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2328 - val_loss: 0.2291\n",
      "Epoch 62\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2326 - val_loss: 0.2282\n",
      "Epoch 63\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2325 - val_loss: 0.2282\n",
      "Epoch 64\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2323 - val_loss: 0.2301\n",
      "Epoch 65\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2322 - val_loss: 0.2286\n",
      "Epoch 66\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2318 - val_loss: 0.2281\n",
      "Epoch 67\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2316 - val_loss: 0.2283\n",
      "Epoch 68\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2315 - val_loss: 0.2283\n",
      "Epoch 69\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2314 - val_loss: 0.2285\n",
      "Epoch 70\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2312 - val_loss: 0.2269\n",
      "Epoch 71\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 65s 3us/sample - loss: 0.2309 - val_loss: 0.2267\n",
      "Epoch 72\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2310 - val_loss: 0.2271\n",
      "Epoch 73\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2307 - val_loss: 0.2255\n",
      "Epoch 74\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2306 - val_loss: 0.2271\n",
      "Epoch 75\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2304 - val_loss: 0.2267\n",
      "Epoch 76\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2302 - val_loss: 0.2275\n",
      "Epoch 77\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2300 - val_loss: 0.2261\n",
      "Epoch 78\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2300 - val_loss: 0.2259\n",
      "Epoch 79\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2299 - val_loss: 0.2256\n",
      "Epoch 80\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2297 - val_loss: 0.2269\n",
      "Epoch 81\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2296 - val_loss: 0.2255\n",
      "Epoch 82\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2294 - val_loss: 0.2248\n",
      "Epoch 83\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2292 - val_loss: 0.2254\n",
      "Epoch 84\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2291 - val_loss: 0.2256\n",
      "Epoch 85\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2289 - val_loss: 0.2260\n",
      "Epoch 86\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 61s 3us/sample - loss: 0.2288 - val_loss: 0.2248\n",
      "Epoch 87\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2287 - val_loss: 0.2255\n",
      "Epoch 88\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2285 - val_loss: 0.2239\n",
      "Epoch 89\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2284 - val_loss: 0.2258\n",
      "Epoch 90\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2284 - val_loss: 0.2244\n",
      "Epoch 91\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2282 - val_loss: 0.2250\n",
      "Epoch 92\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2281 - val_loss: 0.2239\n",
      "Epoch 93\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2280 - val_loss: 0.2250\n",
      "Epoch 94\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 64s 3us/sample - loss: 0.2278 - val_loss: 0.2245\n",
      "Epoch 95\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2279 - val_loss: 0.2232\n",
      "Epoch 96\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 61s 3us/sample - loss: 0.2278 - val_loss: 0.2245\n",
      "Epoch 97\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2276 - val_loss: 0.2250\n",
      "Epoch 98\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2276 - val_loss: 0.2220\n",
      "Epoch 99\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 62s 3us/sample - loss: 0.2274 - val_loss: 0.2233\n",
      "Epoch 100\n",
      "Train on 21500230 samples, validate on 217176 samples\n",
      "21500230/21500230 [==============================] - 63s 3us/sample - loss: 0.2272 - val_loss: 0.2233\n",
      "CPU times: user 4h 31min 55s, sys: 28min 56s, total: 5h 51s\n",
      "Wall time: 1h 58min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    \n",
    "    shuffled_hits_train, shuffled_params_train, outputs_train = shuffle(single_hits_train, repeated_params_train)\n",
    "    shuffled_hits_test, shuffled_params_test, outputs_test = shuffle(single_hits_test, repeated_params_test)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    print('Epoch %i'%(len(loss_history)+1))\n",
    "    \n",
    "    hit_net.fit([shuffled_hits_train, shuffled_params_train], outputs_train, batch_size=1024, epochs=1, validation_data=([shuffled_hits_test, shuffled_params_test], outputs_test)) #, callbacks=[my_history,])\n",
    "\n",
    "    loss_history.append(hit_net.history.history['loss'])\n",
    "    val_loss_history.append(hit_net.history.history['val_loss'])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2a74116410>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "x = np.arange(1, len(loss_history)+1)\n",
    "ax.plot(x, np.concatenate(loss_history), label='train')\n",
    "ax.plot(x, np.concatenate(val_loss_history), label='test')\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "#plt.savefig('train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_net.save('models/new_hit_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hit_net = tf.keras.models.load_model('models/new_hit_net.h5', custom_objects={'trafo_indep':trafo_indep})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 20s 23us/sample - loss: 0.5375 - val_loss: 0.5254\n",
      "Epoch 2\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5339 - val_loss: 0.5259\n",
      "Epoch 3\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 20s 23us/sample - loss: 0.5315 - val_loss: 0.5130\n",
      "Epoch 4\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 20s 23us/sample - loss: 0.5287 - val_loss: 0.5169\n",
      "Epoch 5\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5264 - val_loss: 0.5220\n",
      "Epoch 6\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5252 - val_loss: 0.5115\n",
      "Epoch 7\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5218 - val_loss: 0.5106\n",
      "Epoch 8\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5193 - val_loss: 0.5026\n",
      "Epoch 9\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5177 - val_loss: 0.5085\n",
      "Epoch 10\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5158 - val_loss: 0.5005\n",
      "Epoch 11\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5133 - val_loss: 0.5068\n",
      "Epoch 12\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5111 - val_loss: 0.5029\n",
      "Epoch 13\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5094 - val_loss: 0.4945\n",
      "Epoch 14\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.5063 - val_loss: 0.4933\n",
      "Epoch 15\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5056 - val_loss: 0.4919\n",
      "Epoch 16\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5035 - val_loss: 0.4888\n",
      "Epoch 17\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5021 - val_loss: 0.4934\n",
      "Epoch 18\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.5001 - val_loss: 0.4935\n",
      "Epoch 19\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4993 - val_loss: 0.4831\n",
      "Epoch 20\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4972 - val_loss: 0.4859\n",
      "Epoch 21\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4951 - val_loss: 0.4866\n",
      "Epoch 22\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 22us/sample - loss: 0.4946 - val_loss: 0.4812\n",
      "Epoch 23\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4923 - val_loss: 0.4779\n",
      "Epoch 24\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4906 - val_loss: 0.4739\n",
      "Epoch 25\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4890 - val_loss: 0.4786\n",
      "Epoch 26\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4879 - val_loss: 0.4837\n",
      "Epoch 27\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 22us/sample - loss: 0.4858 - val_loss: 0.4802\n",
      "Epoch 28\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4841 - val_loss: 0.4717\n",
      "Epoch 29\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 22us/sample - loss: 0.4832 - val_loss: 0.4750\n",
      "Epoch 30\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 22us/sample - loss: 0.4822 - val_loss: 0.4757\n",
      "Epoch 31\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4797 - val_loss: 0.4732\n",
      "Epoch 32\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 22us/sample - loss: 0.4793 - val_loss: 0.4686\n",
      "Epoch 33\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4769 - val_loss: 0.4665\n",
      "Epoch 34\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4755 - val_loss: 0.4668\n",
      "Epoch 35\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4741 - val_loss: 0.4605\n",
      "Epoch 36\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 21us/sample - loss: 0.4737 - val_loss: 0.4655\n",
      "Epoch 37\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4719 - val_loss: 0.4653\n",
      "Epoch 38\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4707 - val_loss: 0.4648\n",
      "Epoch 39\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 21us/sample - loss: 0.4697 - val_loss: 0.4607\n",
      "Epoch 40\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4683 - val_loss: 0.4590\n",
      "Epoch 41\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4675 - val_loss: 0.4506\n",
      "Epoch 42\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4661 - val_loss: 0.4610\n",
      "Epoch 43\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4649 - val_loss: 0.4568\n",
      "Epoch 44\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4637 - val_loss: 0.4471\n",
      "Epoch 45\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - ETA: 0s - loss: 0.463 - 19s 22us/sample - loss: 0.4633 - val_loss: 0.4468\n",
      "Epoch 46\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4620 - val_loss: 0.4525\n",
      "Epoch 47\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4604 - val_loss: 0.4454\n",
      "Epoch 48\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4607 - val_loss: 0.4581\n",
      "Epoch 49\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4589 - val_loss: 0.4491\n",
      "Epoch 50\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4590 - val_loss: 0.4443\n",
      "Epoch 51\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4573 - val_loss: 0.4458\n",
      "Epoch 52\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4567 - val_loss: 0.4520\n",
      "Epoch 53\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4558 - val_loss: 0.4461\n",
      "Epoch 54\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4544 - val_loss: 0.4411\n",
      "Epoch 55\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4546 - val_loss: 0.4483\n",
      "Epoch 56\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4543 - val_loss: 0.4465\n",
      "Epoch 57\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4540 - val_loss: 0.4397\n",
      "Epoch 58\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4526 - val_loss: 0.4381\n",
      "Epoch 59\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4508 - val_loss: 0.4415\n",
      "Epoch 60\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4505 - val_loss: 0.4458\n",
      "Epoch 61\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4500 - val_loss: 0.4346\n",
      "Epoch 62\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4485 - val_loss: 0.4376\n",
      "Epoch 63\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4487 - val_loss: 0.4302\n",
      "Epoch 64\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4477 - val_loss: 0.4404\n",
      "Epoch 65\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4469 - val_loss: 0.4440\n",
      "Epoch 66\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4466 - val_loss: 0.4411\n",
      "Epoch 67\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4461 - val_loss: 0.4413\n",
      "Epoch 68\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4459 - val_loss: 0.4328\n",
      "Epoch 69\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4454 - val_loss: 0.4306\n",
      "Epoch 70\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 18s 22us/sample - loss: 0.4437 - val_loss: 0.4345\n",
      "Epoch 71\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4446 - val_loss: 0.4343\n",
      "Epoch 72\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 20s 23us/sample - loss: 0.4433 - val_loss: 0.4332\n",
      "Epoch 73\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4426 - val_loss: 0.4410\n",
      "Epoch 74\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4417 - val_loss: 0.4367\n",
      "Epoch 75\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 20s 23us/sample - loss: 0.4419 - val_loss: 0.4398\n",
      "Epoch 76\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4407 - val_loss: 0.4358\n",
      "Epoch 77\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4407 - val_loss: 0.4386\n",
      "Epoch 78\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4398 - val_loss: 0.4293\n",
      "Epoch 79\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4396 - val_loss: 0.4363\n",
      "Epoch 80\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 22us/sample - loss: 0.4384 - val_loss: 0.4329\n",
      "Epoch 81\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4389 - val_loss: 0.4299\n",
      "Epoch 82\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "851212/851212 [==============================] - 19s 23us/sample - loss: 0.4380 - val_loss: 0.4278\n",
      "Epoch 83\n",
      "Train on 851212 samples, validate on 8600 samples\n",
      "676480/851212 [======================>.......] - ETA: 3s - loss: 0.4378"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    shuffled_q_charges_train, shuffled_q_params_train, outputs_q_train = shuffle(total_charge_train, params_train)\n",
    "    shuffled_q_charges_test, shuffled_q_params_test, outputs_q_test = shuffle(total_charge_test, params_test)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    print('Epoch %i'%(len(loss_history_q)+1))\n",
    "    \n",
    "    charge_net.fit([shuffled_q_charges_train.flatten()[:, np.newaxis], shuffled_q_params_train], outputs_q_train, batch_size=128, epochs=1, validation_data=([shuffled_q_charges_test.flatten()[:, np.newaxis], shuffled_q_params_test], outputs_q_test)) #, callbacks=[my_history_q,])\n",
    "\n",
    "    loss_history_q.append(charge_net.history.history['loss'])\n",
    "    val_loss_history_q.append(charge_net.history.history['val_loss'])\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "x = np.arange(1, len(loss_history_q)+1)\n",
    "ax.plot(x, np.concatenate(loss_history_q), label='train')\n",
    "ax.plot(x, np.concatenate(val_loss_history_q), label='test')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_net.save('models/new_charge_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_net = tf.keras.models.load_model('models/new_charge_net.h5', custom_objects={'trafo_q':trafo_q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
